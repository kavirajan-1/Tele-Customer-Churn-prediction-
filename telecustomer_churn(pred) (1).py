# -*- coding: utf-8 -*-
"""TeleCustomer_churn(pred).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pAddqAayJSaV_YgdU_5TdY_GwNJMLNKP

importing libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split,cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_validate
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

"""Loading Data"""

#load the dataset
df=pd.read_csv("/content/WA_Fn-UseC_-Telco-Customer-Churn.csv")

df.shape

df.head()

pd.set_option("display.max_columns",None)

df.head()

df.info()

df.isnull().sum()

# dropping customer id column as this not required for modelling
df=df.drop(columns=["customerID"])

df.head(2)

print(df["gender"].unique())

# printing the unique values in all the columns

numerical_features_list=["tenure","MonthlyCharges","TotalCharges"]

for col in df.columns:
  if col not in numerical_features_list:
    print(col,df[col].unique())
    print("-"*50)

#df["TotalCharges"]=df["TotalCharges"].astype(float)

df[df["TotalCharges"]==" "]

len(df[df["TotalCharges"]==" "])

df["TotalCharges"]=df["TotalCharges"].replace({" ": "0.0"})

df["TotalCharges"]=df["TotalCharges"].astype(float)

df.info()

# checking the distribution of the target column
print(df["Churn"].value_counts())

"""**Insights:**
1.Customer ID removed it is not required for modelling
2.No missing value in the dataset

Exploratory Data Analysis
"""

df.columns

df.describe()

"""Numerical feature -Analysis

Understand The Distribution of the numerical features
"""

def plot_hisstogram(df,column_name):

  plt.figure(figsize=(5,3))
  sns.histplot(df[column_name],kde=True)
  plt.title(f"Distribution of {column_name}")

  # Calculate the mean and median values for the columns
  col_mean=df[column_name].mean()
  col_median=df[column_name].median()

  #add vertical line for mean and median
  plt.axvline(col_mean,color="red",linestyle="--",label="Mean")
  plt.axvline(col_median,color="green",linestyle="-",label="Median")

  plt.legend()

  plt.show()

plot_hisstogram(df,"tenure")

plot_hisstogram(df,"MonthlyCharges")

plot_hisstogram(df,"TotalCharges")

"""Boxplot for numerical features"""

def plot_boxplot(df,column_name):
  plt.figure(figsize=(5,3))
  sns.boxplot(y=df[column_name])
  plt.title(f"Boxplot of {column_name}")
  plt.ylabel(column_name)
  plt.show()

plot_boxplot(df,"tenure")

plot_boxplot(df,"MonthlyCharges")

plot_boxplot(df,"TotalCharges")

"""Correlation Heatmap for numerical columns"""

#correlation matrix -heatmap
plt.figure(figsize=(8,4))
sns.heatmap(df[["tenure","MonthlyCharges","TotalCharges"]].corr(),annot=True,cmap="coolwarm",fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()

"""Categorical features -Analysis"""

df.columns

df.info()

"""count plot for catogorical columns"""

object_cols=df.select_dtypes(include="object").columns.to_list()
object_cols=["SeniorCitizen"]+object_cols

for col in object_cols:
  plt.figure(figsize=(5,3))
  sns.countplot(x=df[col])
  plt.title(f"Count Plot of {col}")
  plt.show()

"""Data Preprocessing"""

df.head(3)

"""Label encoding of target column"""

df["Churn"]=df["Churn"].replace({"Yes":1,"No":0})

df.head(3)

print(df["Churn"].value_counts())

"""Label encoding of categorical features"""

#Identify The columns With object data type

object_columns=df.select_dtypes(include="object").columns

print(object_columns)

#initialize The dictionary to save encoders

encoders={}

#apply lable encoding and store The encoders

for column in object_columns:
  label_encoder=LabelEncoder()
  df[column]=label_encoder.fit_transform(df[column])
  encoders[column]=label_encoder


#save The encoders to a pickle file

with open("encoders.pkl", "wb") as f:
  pickle.dump(encoders,f)

encoders

df.head(3)

"""**Training and Test data split**"""

# spliting the fatures and target

x=df.drop(columns=["Churn"])
y=df["Churn"]

# split training and test data
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)

print(y_train.shape)

print(y_train.value_counts())

"""Synthetic Minority Oversampling Technique(SMOTE)"""

smote=SMOTE(random_state=0)

x_train_smote,y_train_smote=smote.fit_resample(x_train,y_train)

print(y_train_smote.shape)

print(y_train_smote.value_counts())

"""Model Training

Training with default hyperparameters
"""

# dictionary of models

# dictionary of models

models={
    "Decision Tree": DecisionTreeClassifier(random_state=0),
    "Random Forest": RandomForestClassifier(random_state=0),
    "XGBoost": XGBClassifier(random_state=0)
}

# dictionary to store the cross validation results


#perform 5-fold cross validation for each model
for model_name,model in models.items():
  print(f"Training {model_name} With default parameters")
  scores=cross_val_score(model,x_train_smote,y_train_smote,cv=5,scoring="accuracy")
  cv_scores[model_name]=scores
  print(f"{model_name} cross-validation accuracy: {np.mean(scores):.2f}")

cv_scores

"""Random Forest gives The Highest Accuracy"""

rfc=RandomForestClassifier(random_state=0)

rfc.fit(x_train_smote,y_train_smote)

"""Model Evaluation"""

#evaluation on test data
from sklearn.metrics import classification_report

y_test_pred=rfc.predict(x_test)

print("Accuracy Score :\n",accuracy_score(y_test,y_test_pred))
print("Confusion Matrix :\n",confusion_matrix(y_test,y_test_pred))
print("Classification Report :\n",classification_report(y_test,y_test_pred))

#save the trained model as picke file
model_data={"model":rfc,"features_names":x.columns.tolist()}
with open("customer_churn_model.pkl","wb") as f:
  pickle.dump(rfc,f)

"""Load The saved model and build the predictive system"""

with open("customer_churn_model.pkl","rb") as f:
    model_data=pickle.load(f)

#save the trained model as picke file
model_data={"model":rfc,"features_names":x.columns.tolist()}
with open("customer_churn_model.pkl","wb") as f:
  # Dump the entire model_data dictionary, not just the model
  pickle.dump(model_data,f)

# load The saved model and the feature names

with open("customer_churn_model.pkl","rb") as f:
    model_data=pickle.load(f)

loaded_model=model_data["model"]
feature_names=model_data["features_names"]

print(loaded_model)

print(feature_names)

input_data = {
    'gender': 'Female',
    'SeniorCitizen': 0,
    'Partner': 'Yes',
    'Dependents': 'No',
    'tenure': 1,
    'PhoneService': 'No',
    'MultipleLines': 'No phone service',
    'InternetService': 'DSL',
    'OnlineSecurity': 'No',
    'OnlineBackup': 'Yes',
    'DeviceProtection': 'No',
    'TechSupport': 'No',
    'StreamingTV': 'No',
    'StreamingMovies': 'No',
    'Contract': 'Month-to-month',
    'PaperlessBilling': 'Yes',
    'PaymentMethod': 'Electronic check',
    'MonthlyCharges': 29.85,
    'TotalCharges': 29.85,

}

input_data_df = pd.DataFrame([input_data])

with open("encoders.pkl", "rb") as f:
    encoders = pickle.load(f)


# Encode categorical features using the saved encoders
for column, encoder in encoders.items():
    input_data_df[column] = encoder.transform(input_data_df[column])

# Make a prediction
prediction = loaded_model.predict(input_data_df)
pred_prob = loaded_model.predict_proba(input_data_df)

print(prediction)

# Results
print(f"Prediction: {'Churn' if prediction[0] == 1 else 'No Churn'}")
print(f"Prediction Probability:{pred_prob}")

input_data_df.head()

